{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Generating and Translating Emojis with Generative Adversarial Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmqmaLpDM41w"
      },
      "source": [
        "# Generating and Translating Emojis with Generative Adversarial Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this project we explore the exciting world of generative adversarial networks (GAN) by generating emojis using two different methods and GAN model architctures. \n",
        "\n",
        "In the first part, we implement a specific type of GAN designed to\n",
        "process images, called a Deep Convolutional GAN (DCGAN). We train the DCGAN to generate emojis from samples of random noise.\n",
        "\n",
        "In the second part, we look at a more complex GAN\n",
        "architecture called CycleGAN, which was designed for the task of image-to-image translation. We train the CycleGAN to convert between Apple-style and\n",
        "Windows-style emojis.\n",
        "\n",
        "This project can also be found on my [Github](https://github.com/rizkmena/Generating-and-Translating-Emojis-with-Generative-Adversarial-Networks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJb6z6wDRPLb"
      },
      "source": [
        "# 0. Setup and Helper Code\n",
        "First we get some setup and helper code out of the way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5"
      },
      "source": [
        "## 0.1. PyTorch Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6MQhMOlHXD"
      },
      "source": [
        "######################################################################\n",
        "# Setup python environment and changing the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install imageio\n",
        "\n",
        "!pip install matplotlib\n",
        "\n",
        "%mkdir -p /content/temp/\n",
        "%cd /content/temp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "## 0.2. Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-UJHBYZkh7f"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "\n",
        "import imageio\n",
        "from IPython.display import Image\n",
        "from urllib.error import URLError\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "                \n",
        "def to_var(tensor, cuda=True):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "    \n",
        "def to_data(x):\n",
        "    \"\"\"Converts variable to numpy.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cpu()\n",
        "    return x.data.numpy()\n",
        "\n",
        "\n",
        "def create_dir(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def gan_checkpoint(iteration, G, D, opts):\n",
        "    \"\"\"Saves the parameters of the generator G and discriminator D.\n",
        "    \"\"\"\n",
        "    G_path = os.path.join(opts.checkpoint_dir, 'G.pkl')\n",
        "    D_path = os.path.join(opts.checkpoint_dir, 'D.pkl')\n",
        "    torch.save(G.state_dict(), G_path)\n",
        "    torch.save(D.state_dict(), D_path)\n",
        "\n",
        "\n",
        "def cyclegan_checkpoint(iteration, G_XtoY, G_YtoX, D_X, D_Y, opts):\n",
        "    \"\"\"Saves the parameters of both generators G_YtoX, G_XtoY and discriminators D_X, D_Y.\n",
        "    \"\"\"\n",
        "    G_XtoY_path = os.path.join(opts.checkpoint_dir, 'G_XtoY.pkl')\n",
        "    G_YtoX_path = os.path.join(opts.checkpoint_dir, 'G_YtoX.pkl')\n",
        "    D_X_path = os.path.join(opts.checkpoint_dir, 'D_X.pkl')\n",
        "    D_Y_path = os.path.join(opts.checkpoint_dir, 'D_Y.pkl')\n",
        "    torch.save(G_XtoY.state_dict(), G_XtoY_path)\n",
        "    torch.save(G_YtoX.state_dict(), G_YtoX_path)\n",
        "    torch.save(D_X.state_dict(), D_X_path)\n",
        "    torch.save(D_Y.state_dict(), D_Y_path)\n",
        "\n",
        "\n",
        "def load_checkpoint(opts):\n",
        "    \"\"\"Loads the generator and discriminator models from checkpoints.\n",
        "    \"\"\"\n",
        "    G_XtoY_path = os.path.join(opts.load, 'G_XtoY.pkl')\n",
        "    G_YtoX_path = os.path.join(opts.load, 'G_YtoX.pkl')\n",
        "    D_X_path = os.path.join(opts.load, 'D_X.pkl')\n",
        "    D_Y_path = os.path.join(opts.load, 'D_Y.pkl')\n",
        "\n",
        "    G_XtoY = CycleGenerator(conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights)\n",
        "    G_YtoX = CycleGenerator(conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights)\n",
        "    D_X = DCDiscriminator(conv_dim=opts.d_conv_dim)\n",
        "    D_Y = DCDiscriminator(conv_dim=opts.d_conv_dim)\n",
        "\n",
        "    G_XtoY.load_state_dict(torch.load(G_XtoY_path, map_location=lambda storage, loc: storage))\n",
        "    G_YtoX.load_state_dict(torch.load(G_YtoX_path, map_location=lambda storage, loc: storage))\n",
        "    D_X.load_state_dict(torch.load(D_X_path, map_location=lambda storage, loc: storage))\n",
        "    D_Y.load_state_dict(torch.load(D_Y_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        G_XtoY.cuda()\n",
        "        G_YtoX.cuda()\n",
        "        D_X.cuda()\n",
        "        D_Y.cuda()\n",
        "        print('Models moved to GPU.')\n",
        "\n",
        "    return G_XtoY, G_YtoX, D_X, D_Y\n",
        "\n",
        "\n",
        "def merge_images(sources, targets, opts):\n",
        "    \"\"\"Creates a grid consisting of pairs of columns, where the first column in\n",
        "    each pair contains images source images and the second column in each pair\n",
        "    contains images generated by the CycleGAN from the corresponding images in\n",
        "    the first column.\n",
        "    \"\"\"\n",
        "    _, _, h, w = sources.shape\n",
        "    row = int(np.sqrt(opts.batch_size))\n",
        "    merged = np.zeros([3, row * h, row * w * 2])\n",
        "    for (idx, s, t) in (zip(range(row ** 2), sources, targets, )):\n",
        "        i = idx // row\n",
        "        j = idx % row\n",
        "        merged[:, i * h:(i + 1) * h, (j * 2) * h:(j * 2 + 1) * h] = s\n",
        "        merged[:, i * h:(i + 1) * h, (j * 2 + 1) * h:(j * 2 + 2) * h] = t\n",
        "    return merged.transpose(1, 2, 0)\n",
        "\n",
        "\n",
        "def generate_gif(directory_path, keyword=None):\n",
        "    images = []\n",
        "    for filename in sorted(os.listdir(directory_path)):\n",
        "        if filename.endswith(\".png\") and (keyword is None or keyword in filename):\n",
        "            img_path = os.path.join(directory_path, filename)\n",
        "            print(\"adding image {}\".format(img_path))\n",
        "            images.append(imageio.imread(img_path))\n",
        "\n",
        "    if keyword:\n",
        "        imageio.mimsave(\n",
        "            os.path.join(directory_path, 'anim_{}.gif'.format(keyword)), images)\n",
        "    else:\n",
        "        imageio.mimsave(os.path.join(directory_path, 'anim.gif'), images)\n",
        "\n",
        "\n",
        "def create_image_grid(array, ncols=None):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    num_images, channels, cell_h, cell_w = array.shape\n",
        "    if not ncols:\n",
        "        ncols = int(np.sqrt(num_images))\n",
        "    nrows = int(np.math.floor(num_images / float(ncols)))\n",
        "    result = np.zeros((cell_h * nrows, cell_w * ncols, channels), dtype=array.dtype)\n",
        "    for i in range(0, nrows):\n",
        "        for j in range(0, ncols):\n",
        "            result[i * cell_h:(i + 1) * cell_h, j * cell_w:(j + 1) * cell_w, :] = array[i * ncols + j].transpose(1, 2,\n",
        "                                                                                                                 0)\n",
        "\n",
        "    if channels == 1:\n",
        "        result = result.squeeze()\n",
        "    return result\n",
        "\n",
        "\n",
        "def gan_save_samples(G, fixed_noise, iteration, opts):\n",
        "    generated_images = G(fixed_noise)\n",
        "    generated_images = to_data(generated_images)\n",
        "\n",
        "    grid = create_image_grid(generated_images)\n",
        "\n",
        "    # merged = merge_images(X, fake_Y, opts)\n",
        "    path = os.path.join(opts.sample_dir, 'sample-{:06d}.png'.format(iteration))\n",
        "    imageio.imwrite(path, grid)\n",
        "    print('Saved {}'.format(path))\n",
        "\n",
        "\n",
        "def cyclegan_save_samples(iteration, fixed_Y, fixed_X, G_YtoX, G_XtoY, opts):\n",
        "    \"\"\"Saves samples from both generators X->Y and Y->X.\n",
        "    \"\"\"\n",
        "    fake_X = G_YtoX(fixed_Y)\n",
        "    fake_Y = G_XtoY(fixed_X)\n",
        "\n",
        "    X, fake_X = to_data(fixed_X), to_data(fake_X)\n",
        "    Y, fake_Y = to_data(fixed_Y), to_data(fake_Y)\n",
        "\n",
        "    merged = merge_images(X, fake_Y, opts)\n",
        "    path = os.path.join(opts.sample_dir, 'sample-{:06d}-X-Y.png'.format(iteration))\n",
        "    imageio.imwrite(path, merged)\n",
        "    print('Saved {}'.format(path))\n",
        "\n",
        "    merged = merge_images(Y, fake_X, opts)\n",
        "    path = os.path.join(opts.sample_dir, 'sample-{:06d}-Y-X.png'.format(iteration))\n",
        "    imageio.imwrite(path, merged)\n",
        "    print('Saved {}'.format(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1"
      },
      "source": [
        "## 0.3. Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVT4TNTOV3Eg"
      },
      "source": [
        "def get_emoji_loader(emoji_type, opts):\n",
        "    \"\"\"Creates training and test data loaders.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "                    transforms.Scale(opts.image_size),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                ])\n",
        "\n",
        "    train_path = os.path.join('data/emojis', emoji_type)\n",
        "    test_path = os.path.join('data/emojis', 'Test_{}'.format(emoji_type))\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(train_path, transform)\n",
        "    test_dataset = datasets.ImageFolder(test_path, transform)\n",
        "\n",
        "    train_dloader = DataLoader(dataset=train_dataset, batch_size=opts.batch_size, shuffle=True, num_workers=opts.num_workers)\n",
        "    test_dloader = DataLoader(dataset=test_dataset, batch_size=opts.batch_size, shuffle=False, num_workers=opts.num_workers)\n",
        "\n",
        "    return train_dloader, test_dloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF59v7ssEfQo"
      },
      "source": [
        "datadir = os.path.join('data')\n",
        "if not os.path.exists(datadir):\n",
        "    os.makedirs(datadir)\n",
        "with tarfile.open('emojis.tar.gz') as archive:\n",
        "    archive.extractall(datadir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl"
      },
      "source": [
        "## 0.4. Training and Evaluation Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSIhQp41q_Nu"
      },
      "source": [
        "def print_models(G_XtoY, G_YtoX, D_X, D_Y):\n",
        "    \"\"\"Prints model information for the generators and discriminators.\n",
        "    \"\"\"\n",
        "    if G_YtoX:\n",
        "        print(\"                 G_XtoY                \")\n",
        "        print(\"---------------------------------------\")\n",
        "        print(G_XtoY)\n",
        "        print(\"---------------------------------------\")\n",
        "\n",
        "        print(\"                 G_YtoX                \")\n",
        "        print(\"---------------------------------------\")\n",
        "        print(G_YtoX)\n",
        "        print(\"---------------------------------------\")\n",
        "\n",
        "        print(\"                  D_X                  \")\n",
        "        print(\"---------------------------------------\")\n",
        "        print(D_X)\n",
        "        print(\"---------------------------------------\")\n",
        "\n",
        "        print(\"                  D_Y                  \")\n",
        "        print(\"---------------------------------------\")\n",
        "        print(D_Y)\n",
        "        print(\"---------------------------------------\")\n",
        "    else:\n",
        "        print(\"                 G                     \")\n",
        "        print(\"---------------------------------------\")\n",
        "        print(G_XtoY)\n",
        "        print(\"---------------------------------------\")\n",
        "\n",
        "        print(\"                  D                    \")\n",
        "        print(\"---------------------------------------\")\n",
        "        print(D_X)\n",
        "        print(\"---------------------------------------\")\n",
        "\n",
        "\n",
        "def create_model(opts):\n",
        "    \"\"\"Builds the generators and discriminators.\n",
        "    \"\"\"\n",
        "    if opts.Y is None:\n",
        "        ### GAN\n",
        "        G = DCGenerator(noise_size=opts.noise_size, conv_dim=opts.g_conv_dim, spectral_norm=opts.spectral_norm)\n",
        "        D = DCDiscriminator(conv_dim=opts.d_conv_dim, spectral_norm=opts.spectral_norm)\n",
        "\n",
        "        print_models(G, None, D, None)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            G.cuda()\n",
        "            D.cuda()\n",
        "            print('Models moved to GPU.')\n",
        "        return G, D\n",
        "          \n",
        "    else:\n",
        "        ### CycleGAN\n",
        "        G_XtoY = CycleGenerator(conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights)\n",
        "        G_YtoX = CycleGenerator(conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights)\n",
        "        D_X = DCDiscriminator(conv_dim=opts.d_conv_dim)\n",
        "        D_Y = DCDiscriminator(conv_dim=opts.d_conv_dim)\n",
        "\n",
        "        print_models(G_XtoY, G_YtoX, D_X, D_Y)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            G_XtoY.cuda()\n",
        "            G_YtoX.cuda()\n",
        "            D_X.cuda()\n",
        "            D_Y.cuda()\n",
        "            print('Models moved to GPU.')\n",
        "        return G_XtoY, G_YtoX, D_X, D_Y\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    \"\"\"Loads the data, creates checkpoint and sample directories, and starts the training loop.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create train and test dataloaders for images from the two domains X and Y\n",
        "    dataloader_X, test_dataloader_X = get_emoji_loader(emoji_type=opts.X, opts=opts)\n",
        "    if opts.Y:\n",
        "        dataloader_Y, test_dataloader_Y = get_emoji_loader(emoji_type=opts.Y, opts=opts)\n",
        "\n",
        "    # Create checkpoint and sample directories\n",
        "    create_dir(opts.checkpoint_dir)\n",
        "    create_dir(opts.sample_dir)\n",
        "\n",
        "    # Start training\n",
        "    if opts.Y is None:\n",
        "        G, D = gan_training_loop(dataloader_X, test_dataloader_X, opts)\n",
        "        return G, D\n",
        "    else:\n",
        "        G_XtoY, G_YtoX, D_X, D_Y = cyclegan_training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, opts)\n",
        "        return G_XtoY, G_YtoX, D_X, D_Y\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        if opts.__dict__[key]:\n",
        "            print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0KxX0sDpXKb"
      },
      "source": [
        "## 0.5. Additional Helper Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7s0etAmpUgT"
      },
      "source": [
        "def sample_noise(batch_size, dim):\n",
        "    \"\"\"\n",
        "    Generate a PyTorch Tensor of uniform random noise.\n",
        "\n",
        "    Input:\n",
        "    - batch_size: Integer giving the batch size of noise to generate.\n",
        "    - dim: Integer giving the dimension of noise to generate.\n",
        "\n",
        "    Output:\n",
        "    - A PyTorch Tensor of shape (batch_size, dim, 1, 1) containing uniform\n",
        "      random noise in the range (-1, 1).\n",
        "    \"\"\"\n",
        "    return to_var(torch.rand(batch_size, dim) * 2 - 1).unsqueeze(2).unsqueeze(3)\n",
        "  \n",
        "\n",
        "def upconv(in_channels, out_channels, kernel_size, stride=2, padding=2, batch_norm=True, spectral_norm=False):\n",
        "    \"\"\"Creates a upsample-and-convolution layer, with optional batch normalization.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    if stride>1:\n",
        "        layers.append(nn.Upsample(scale_factor=stride))\n",
        "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False)\n",
        "    if spectral_norm:\n",
        "        layers.append(SpectralNorm(conv_layer))\n",
        "    else:\n",
        "        layers.append(conv_layer)\n",
        "    if batch_norm:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def conv(in_channels, out_channels, kernel_size, stride=2, padding=2, batch_norm=True, init_zero_weights=False, spectral_norm=False):\n",
        "    \"\"\"Creates a convolutional layer, with optional batch normalization.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
        "    if init_zero_weights:\n",
        "        conv_layer.weight.data = torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.001\n",
        "            \n",
        "    if spectral_norm:\n",
        "        layers.append(SpectralNorm(conv_layer))\n",
        "    else:\n",
        "        layers.append(conv_layer)\n",
        "\n",
        "    if batch_norm:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "    return nn.Sequential(*layers)\n",
        "  \n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, conv_dim):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_layer = conv(in_channels=conv_dim, out_channels=conv_dim, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_layer(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0_YbBwe5k35"
      },
      "source": [
        "#Part 1: DCGAN\n",
        "In this section, we implement a *Deep Convolutional GAN* (DCGAN). A\n",
        "DCGAN is simply a GAN that uses a convolutional neural network as the discriminator, and a\n",
        "network composed of *transposed convolutions* as the generator. To implement the DCGAN, we need\n",
        "to specify three things: 1) the generator, 2) the discriminator, and 3) the training procedure. Before implementing, we briefly go over each of these three components in their respective sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1E_jDaBLT1P"
      },
      "source": [
        "## 1.1. Spectral Norm Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hy97i1-LT1Q"
      },
      "source": [
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BAfi_8yWB3y"
      },
      "source": [
        "## 1.2. GAN Generator\n",
        "The generator of the DCGAN consists of a sequence of transpose convolutional layers that progressively upsample the input noise sample to generate a fake image. The generator's architecture is depicted in the diagram below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyyju-HpZmsN"
      },
      "source": [
        "![Fig1](https://drive.google.com/uc?id=1zUSTpqoEUkvxoQCHxGjm-ZvEJpUsr9_p)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ztmyA5Ro67o"
      },
      "source": [
        "class DCGenerator(nn.Module):\n",
        "    def __init__(self, noise_size, conv_dim, spectral_norm=False):\n",
        "        super(DCGenerator, self).__init__()\n",
        "\n",
        "        self.conv_dim = conv_dim\n",
        "        self.linear_bn = upconv(noise_size, conv_dim*4, 5, stride=4)\n",
        "        self.upconv1 = upconv(conv_dim*4, conv_dim*2, 5)\n",
        "        self.upconv2 = upconv(conv_dim*2, conv_dim, 5)\n",
        "        self.upconv3 = upconv(conv_dim, 3, 5, batch_norm=False)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"Generates an image given a sample of random noise.\n",
        "\n",
        "            Input\n",
        "            -----\n",
        "                z: BS x noise_size x 1 x 1   -->  BSx100x1x1 (during training)\n",
        "\n",
        "            Output\n",
        "            ------\n",
        "                out: BS x channels x image_width x image_height  -->  BSx3x32x32 (during training)\n",
        "        \"\"\"\n",
        "        batch_size = z.size(0)\n",
        "        \n",
        "        out = F.relu(self.linear_bn(z)).view(-1, self.conv_dim*4, 4, 4)    # BS x 128 x 4 x 4\n",
        "        out = F.relu(self.upconv1(out))  # BS x 64 x 8 x 8\n",
        "        out = F.relu(self.upconv2(out))  # BS x 32 x 16 x 16\n",
        "        out = F.tanh(self.upconv3(out))  # BS x 3 x 32 x 32\n",
        "        \n",
        "        out_size = out.size()\n",
        "        if out_size != torch.Size([batch_size, 3, 32, 32]):\n",
        "            raise ValueError(\"expect {} x 3 x 32 x 32, but get {}\".format(batch_size, out_size))\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG4uqAVPp8_B"
      },
      "source": [
        "## 1.3. GAN Discriminator\n",
        "The discriminator of the DCGAN is now implemented with the architecture depicted below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0f7MWSEZ9kE"
      },
      "source": [
        "![Fig2](https://drive.google.com/uc?id=1PU0NDxtnNLGCqflRtwqYe9t3YTO7cRec)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GkjXydnqARR"
      },
      "source": [
        "class DCDiscriminator(nn.Module):\n",
        "    \"\"\"Defines the architecture of the discriminator network.\n",
        "       Note: Both discriminators D_X and D_Y have the same architecture in this assignment.\n",
        "    \"\"\"\n",
        "    def __init__(self, conv_dim=64, spectral_norm=False):\n",
        "        super(DCDiscriminator, self).__init__()\n",
        "\n",
        "        self.conv1 = conv(in_channels=3, out_channels=conv_dim, kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
        "        self.conv2 = conv(in_channels=conv_dim, out_channels=conv_dim*2, kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
        "        self.conv3 = conv(in_channels=conv_dim*2, out_channels=conv_dim*4, kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
        "        self.conv4 = conv(in_channels=conv_dim*4, out_channels=1, kernel_size=5, stride=2, padding=1, batch_norm=False, spectral_norm=spectral_norm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        out = F.relu(self.conv1(x))    # BS x 64 x 16 x 16\n",
        "        out = F.relu(self.conv2(out))    # BS x 64 x 8 x 8\n",
        "        out = F.relu(self.conv3(out))    # BS x 64 x 4 x 4\n",
        "\n",
        "        out = self.conv4(out).squeeze()\n",
        "        out_size = out.size()\n",
        "        if out_size != torch.Size([batch_size,]):\n",
        "            raise ValueError(\"expect {} x 1, but get {}\".format(batch_size, out_size))\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8RtBMu55ysm"
      },
      "source": [
        "## 1.4. GAN Training Loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF0n2CyMbdkr"
      },
      "source": [
        "We now implement the training loop for the DCGAN, which is an implementation of the pseudo-code below. Note that in our implementation of the discriminator update, we add a gradient penalty  term to the discriminator loss. This is a popular technique for stabilizing GAN training that can take different forms and is active research area in its effects on GAN training. [Gulrajani et al., 2017] [Kodali\n",
        "et al., 2017] [Mescheder et al., 2018]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-wgUxGIaUQr"
      },
      "source": [
        "![Fig3](https://drive.google.com/uc?id=1oEHohkMzMjDQMs1iwCuAImkRBcP9tVks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxIJ2Zua51KI"
      },
      "source": [
        "def gan_training_loop(dataloader, test_dataloader, opts):\n",
        "    \"\"\"Runs the training loop.\n",
        "        * Saves checkpoint every opts.checkpoint_every iterations\n",
        "        * Saves generated samples every opts.sample_every iterations\n",
        "    \"\"\"\n",
        "\n",
        "    # Create generators and discriminators\n",
        "    G, D = create_model(opts)\n",
        "\n",
        "    g_params = G.parameters()  # Get generator parameters\n",
        "    d_params = D.parameters()  # Get discriminator parameters\n",
        "\n",
        "    # Create optimizers for the generators and discriminators\n",
        "    g_optimizer = optim.Adam(g_params, opts.lr, [opts.beta1, opts.beta2])\n",
        "    d_optimizer = optim.Adam(d_params, opts.lr * 2., [opts.beta1, opts.beta2])\n",
        "\n",
        "    train_iter = iter(dataloader)\n",
        "\n",
        "    test_iter = iter(test_dataloader)\n",
        "\n",
        "    # Get some fixed data from domains X and Y for sampling. These are images that are held\n",
        "    # constant throughout training, that allow us to inspect the model's performance.\n",
        "    fixed_noise = sample_noise(100, opts.noise_size)  # # 100 x noise_size x 1 x 1\n",
        "\n",
        "    iter_per_epoch = len(train_iter)\n",
        "    total_train_iters = opts.train_iters\n",
        "\n",
        "    losses = {\"iteration\": [], \"D_fake_loss\": [], \"D_real_loss\": [], \"G_loss\": []}\n",
        "\n",
        "    gp_weight = 10\n",
        "\n",
        "    try:\n",
        "        for iteration in range(1, opts.train_iters + 1):\n",
        "\n",
        "            # Reset data_iter for each epoch\n",
        "            if iteration % iter_per_epoch == 0:\n",
        "                train_iter = iter(dataloader)\n",
        "\n",
        "            real_images, real_labels = train_iter.next()\n",
        "            real_images, real_labels = to_var(real_images), to_var(real_labels).long().squeeze()\n",
        "\n",
        "            # ones = Variable(torch.Tensor(real_images.shape[0]).float().cuda().fill_(1.0), requires_grad=False)\n",
        "\n",
        "            for d_i in range(opts.d_train_iters):\n",
        "                d_optimizer.zero_grad()\n",
        "\n",
        "                # 1. Compute the discriminator loss on real images\n",
        "                D_real_loss = (1/2)*torch.mean((D(real_images) - 1)**2)\n",
        "\n",
        "                # 2. Sample noise\n",
        "                noise = sample_noise(real_images.shape[0], opts.noise_size)\n",
        "\n",
        "                # 3. Generate fake images from the noise\n",
        "                fake_images = G(noise)\n",
        "                \n",
        "                # 4. Compute the discriminator loss on the fake images\n",
        "                D_fake_loss = (1/2)*torch.mean((D(fake_images))**2)\n",
        "\n",
        "                # ---- Gradient Penalty ----\n",
        "                if opts.gradient_penalty:\n",
        "                    alpha = torch.rand(real_images.shape[0], 1, 1, 1)\n",
        "                    alpha = alpha.expand_as(real_images).cuda()\n",
        "                    interp_images = Variable(alpha * real_images.data + (1-alpha) * fake_images.data, requires_grad=True).cuda()\n",
        "                    D_interp_output = D(interp_images)\n",
        "\n",
        "                    gradients = torch.autograd.grad(outputs=D_interp_output, inputs=interp_images,\n",
        "                                                    grad_outputs=torch.ones(D_interp_output.size()).cuda(),\n",
        "                                                    create_graph=True, retain_graph=True)[0]\n",
        "                    gradients = gradients.view(real_images.shape[0], -1)\n",
        "                    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
        "\n",
        "                    gp = gp_weight * gradients_norm.mean()\n",
        "                else:\n",
        "                    gp = 0.0\n",
        "                # --------------------------\n",
        "                \n",
        "                # 5. Compute the total discriminator loss\n",
        "                D_total_loss = D_real_loss + D_fake_loss\n",
        "\n",
        "                D_total_loss.backward()\n",
        "                d_optimizer.step()\n",
        "\n",
        "            g_optimizer.zero_grad()\n",
        "            \n",
        "            # 1. Sample noise\n",
        "            noise = sample_noise(real_images.shape[0], opts.noise_size)\n",
        "\n",
        "            # 2. Generate fake images from the noise\n",
        "            fake_images = G(noise)\n",
        "\n",
        "            # 3. Compute the generator loss\n",
        "            G_loss = torch.mean((D(fake_images) - 1)**2)\n",
        "\n",
        "            G_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            # Print the log info\n",
        "            if iteration % opts.log_step == 0:\n",
        "                losses['iteration'].append(iteration)\n",
        "                losses['D_real_loss'].append(D_real_loss.item())\n",
        "                losses['D_fake_loss'].append(D_fake_loss.item())\n",
        "                losses['G_loss'].append(G_loss.item())\n",
        "                print('Iteration [{:4d}/{:4d}] | D_real_loss: {:6.4f} | D_fake_loss: {:6.4f} | G_loss: {:6.4f}'.format(\n",
        "                    iteration, total_train_iters, D_real_loss.item(), D_fake_loss.item(), G_loss.item()))\n",
        "\n",
        "            # Save the generated samples\n",
        "            if iteration % opts.sample_every == 0:\n",
        "                gan_save_samples(G, fixed_noise, iteration, opts)\n",
        "\n",
        "            # Save the model parameters\n",
        "            if iteration % opts.checkpoint_every == 0:\n",
        "                gan_checkpoint(iteration, G, D, opts)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return G, D\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(losses['iteration'], losses['D_real_loss'], label='D_real')\n",
        "    plt.plot(losses['iteration'], losses['D_fake_loss'], label='D_fake')\n",
        "    plt.plot(losses['iteration'], losses['G_loss'], label='G')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(opts.sample_dir, 'losses.png'))\n",
        "    plt.close()\n",
        "    return G, D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptSA0rHUuNHa"
      },
      "source": [
        "#Part 2: CycleGAN\n",
        "\n",
        "Now, instead of developing a model for creating brand-new emojis, we shift our attention to a GAN model that takes in as input either an Apple or Windows version of an emoji and outputs the equivalent emoji in the other's style.\n",
        "\n",
        "But first, we take a moment to discuss the broader context of this class of models. The core concept beind CycleGAN is that of image-to-image translation. That is, using a conditional GAN to learn a mapping from input to output images. The loss functions of these approaches generally include extra terms to constrain the types of images that are generated.\n",
        "\n",
        "CycleGAN is a recently introduced method for image-to-image translation that enbles us to use un-paired traning data, meaning that we can learn to translate images from one domain to another without having an exact correspondence between the individual images in both domains. The diagram below outlines the core CycleGAN components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo-q__LmhTVw"
      },
      "source": [
        "![Fig4](https://drive.google.com/uc?id=1ja7EC1IrP67a2FIFQuTO2Fqff9211CNn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8lIyRNGuOrv"
      },
      "source": [
        "## 2.1. CycleGAN Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPrSHO1lgBjH"
      },
      "source": [
        "The generator in the CycleGAN has layers that implement three stages of computation: 1) the first\n",
        "stage *encodes* the input via a series of convolutional layers that extract the image features; 2) the\n",
        "second stage then *transforms* the features by passing them through one or more *residual blocks*;\n",
        "and 3) the third stage *decodes* the transformed features using a series of transpose convolutional\n",
        "layers, to build an output image of the same size as the input.\n",
        "\n",
        "The residual block used in the transformation stage consists of a convolutional layer, where the\n",
        "input is added to the output of the convolution. This is done so that the characteristics of the\n",
        "output image (e.g., the shapes of objects) do not differ too much from the input. \n",
        "\n",
        "The below diagram depicts the generator's architecture. Note that we implement two generators in the CycleGAN model ($G_{X→Y}$ and $G_{Y→X}$) such that we are able to produce Apple → Windows and Windows → Apple translations. Both generators are identical in structure, however.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daSGzUCtgXeZ"
      },
      "source": [
        "![Fig5](https://drive.google.com/uc?id=1zwiQxTMTs0gIL-patIG8mIoVkPGN9TpN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVswO-M-uXLo"
      },
      "source": [
        "class CycleGenerator(nn.Module):\n",
        "    \"\"\"Defines the architecture of the generator network.\n",
        "       Note: Both generators G_XtoY and G_YtoX have the same architecture in this assignment.\n",
        "    \"\"\"\n",
        "    def __init__(self, conv_dim=64, init_zero_weights=False):\n",
        "        super(CycleGenerator, self).__init__()\n",
        "\n",
        "        # 1. Define the encoder part of the generator (that extracts features from the input image)\n",
        "        self.conv1 = conv(in_channels=3, out_channels=conv_dim, kernel_size=5, init_zero_weights=init_zero_weights)\n",
        "        self.conv2 = conv(in_channels=conv_dim, out_channels=conv_dim*2, kernel_size=5, init_zero_weights=init_zero_weights)\n",
        "\n",
        "        # 2. Define the transformation part of the generator\n",
        "        self.resnet_block  = ResnetBlock(conv_dim*2)\n",
        "\n",
        "        # 3. Define the decoder part of the generator (that builds up the output image from features)\n",
        "        self.upconv1 = upconv(in_channels=conv_dim*2, out_channels=conv_dim, kernel_size=5)\n",
        "        self.upconv2 = upconv(in_channels=conv_dim, out_channels=3, kernel_size=5, batch_norm=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Generates an image conditioned on an input image.\n",
        "\n",
        "            Input\n",
        "            -----\n",
        "                x: BS x 3 x 32 x 32\n",
        "\n",
        "            Output\n",
        "            ------\n",
        "                out: BS x 3 x 32 x 32\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        out = F.relu(self.conv1(x))            # BS x 32 x 16 x 16\n",
        "        out = F.relu(self.conv2(out))          # BS x 64 x 8 x 8\n",
        "        out = F.relu(self.resnet_block(out))   # BS x 64 x 8 x 8\n",
        "        out = F.relu(self.upconv1(out))        # BS x 32 x 16 x 16\n",
        "        out = F.tanh(self.upconv2(out))        # BS x 3 x 32 x 32\n",
        "        out_size = out.size()\n",
        "        \n",
        "        if out_size != torch.Size([batch_size, 3, 32, 32]):\n",
        "            raise ValueError(\"expect {} x 3 x 32 x 32, but get {}\".format(batch_size, out_size))\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYVbPQ6OuB1w"
      },
      "source": [
        "## 2.2. CycleGAN Training Loop\n",
        "\n",
        "We now implement the training loop for the CycleGAN model as per the pseudo-code below. Training the CycleGAN involves using a *cycle consistency loss* (which gives this model its name) to constrain the model. The idea is that when we\n",
        "translate an image from domain $X$ to domain $Y$ , and then translate the generated image *back* to\n",
        "domain $X$, the result should look like the original image that we started with.\n",
        "\n",
        "The cycle consistency component of the loss is the L1 distance between the input images and\n",
        "their *reconstructions* obtained by passing through both generators in sequence (i.e., from domain\n",
        "$X$ to $Y$ via the $X → Y$ generator, and then from domain $Y$ back to $X$ via the $Y → X$ generator).\n",
        "The cycle consistency loss for the $Y → X → Y$ cycle is expressed as follows:\n",
        "\n",
        "<center>$\\lambda_{\\text {cycle }} \\mathcal{J}_{\\text {cycle }}^{(Y \\rightarrow X \\rightarrow Y)}=\\lambda_{\\text {cycle }} \\frac{1}{m} \\sum_{i=1}^{m}\\left\\|y^{(i)}-G_{X \\rightarrow Y}\\left(G_{Y \\rightarrow X}\\left(y^{(i)}\\right)\\right)\\right\\|_{1}$,</center>\n",
        "\n",
        "where $\\lambda_{\\text {cycle }}$ is a scalar hyper-parameter balancing the two loss terms: the cycle consistant loss and\n",
        "the GAN loss. The loss for the $X → Y → X$ cycle is analogous."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJaEpmtdqAQA"
      },
      "source": [
        "![Fig5](https://drive.google.com/uc?id=1z7C0Gk0vS86MIuShSIqX4mMEKe0AhwEp)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFfQjxstuGaA"
      },
      "source": [
        "def cyclegan_training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, opts):\n",
        "    \"\"\"Runs the training loop.\n",
        "        * Saves checkpoint every opts.checkpoint_every iterations\n",
        "        * Saves generated samples every opts.sample_every iterations\n",
        "    \"\"\"\n",
        "\n",
        "    # Create generators and discriminators\n",
        "    G_XtoY, G_YtoX, D_X, D_Y = create_model(opts)\n",
        "\n",
        "    g_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters())  # Get generator parameters\n",
        "    d_params = list(D_X.parameters()) + list(D_Y.parameters())  # Get discriminator parameters\n",
        "\n",
        "    # Create optimizers for the generators and discriminators\n",
        "    g_optimizer = optim.Adam(g_params, opts.lr, [opts.beta1, opts.beta2])\n",
        "    d_optimizer = optim.Adam(d_params, opts.lr, [opts.beta1, opts.beta2])\n",
        "\n",
        "    iter_X = iter(dataloader_X)\n",
        "    iter_Y = iter(dataloader_Y)\n",
        "\n",
        "    test_iter_X = iter(test_dataloader_X)\n",
        "    test_iter_Y = iter(test_dataloader_Y)\n",
        "\n",
        "    # Get some fixed data from domains X and Y for sampling. These are images that are held\n",
        "    # constant throughout training, that allow us to inspect the model's performance.\n",
        "    fixed_X = to_var(test_iter_X.next()[0])\n",
        "    fixed_Y = to_var(test_iter_Y.next()[0])\n",
        "\n",
        "    iter_per_epoch = min(len(iter_X), len(iter_Y))\n",
        "\n",
        "    try:\n",
        "        for iteration in range(1, opts.train_iters+1):\n",
        "\n",
        "            # Reset data_iter for each epoch\n",
        "            if iteration % iter_per_epoch == 0:\n",
        "                iter_X = iter(dataloader_X)\n",
        "                iter_Y = iter(dataloader_Y)\n",
        "\n",
        "            images_X, labels_X = iter_X.next()\n",
        "            images_X, labels_X = to_var(images_X), to_var(labels_X).long().squeeze()\n",
        "\n",
        "            images_Y, labels_Y = iter_Y.next()\n",
        "            images_Y, labels_Y = to_var(images_Y), to_var(labels_Y).long().squeeze()\n",
        "\n",
        "\n",
        "            # ============================================\n",
        "            #            TRAIN THE DISCRIMINATORS\n",
        "            # ============================================\n",
        "\n",
        "            # Train with real images\n",
        "            d_optimizer.zero_grad()\n",
        "\n",
        "            # 1. Compute the discriminator losses on real images\n",
        "            D_X_loss = torch.mean((D_X(images_X) - 1)**2)\n",
        "            D_Y_loss = torch.mean((D_Y(images_Y) - 1)**2)\n",
        "\n",
        "            d_real_loss = D_X_loss + D_Y_loss\n",
        "            d_real_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # Train with fake images\n",
        "            d_optimizer.zero_grad()\n",
        "\n",
        "            # 2. Generate fake images that look like domain X based on real images in domain Y\n",
        "            fake_X = G_YtoX(images_Y)\n",
        "\n",
        "            # 3. Compute the loss for D_X\n",
        "            D_X_loss = torch.mean(D_X(fake_X)**2)\n",
        "\n",
        "            # 4. Generate fake images that look like domain Y based on real images in domain X\n",
        "            fake_Y = G_XtoY(images_X)\n",
        "\n",
        "            # 5. Compute the loss for D_Y\n",
        "            D_Y_loss = torch.mean(D_Y(fake_Y)**2)\n",
        "\n",
        "            d_fake_loss = D_X_loss + D_Y_loss\n",
        "            d_fake_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # =========================================\n",
        "            #            TRAIN THE GENERATORS\n",
        "            # =========================================\n",
        "\n",
        "            g_optimizer.zero_grad()\n",
        "\n",
        "            # 1. Generate fake images that look like domain X based on real images in domain Y\n",
        "            fake_X = G_YtoX(images_Y)\n",
        "\n",
        "            # 2. Compute the generator loss based on domain X\n",
        "            g_loss = torch.mean((D_X(fake_X) - 1)**2)\n",
        "\n",
        "            reconstructed_Y = G_XtoY(fake_X)\n",
        "            # 3. Compute the cycle consistency loss (the reconstruction loss)\n",
        "            cycle_consistency_loss = torch.mean(torch.sum(torch.abs(images_Y - reconstructed_Y), [1,2,3]))\n",
        "\n",
        "            g_loss += opts.lambda_cycle * cycle_consistency_loss\n",
        "\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "            g_optimizer.zero_grad()\n",
        "\n",
        "            # 1. Generate fake images that look like domain Y based on real images in domain X\n",
        "            # fake_Y = ...\n",
        "            fake_Y = G_XtoY(images_X)\n",
        "\n",
        "            # 2. Compute the generator loss based on domain Y\n",
        "            # g_loss = ...\n",
        "            g_loss = torch.mean((D_Y(fake_Y) - 1)**2)\n",
        "\n",
        "            reconstructed_X = G_YtoX(fake_Y)\n",
        "            # 3. Compute the cycle consistency loss (the reconstruction loss)\n",
        "            # cycle_consistency_loss = ...\n",
        "            cycle_consistency_loss = torch.mean(torch.sum(torch.abs(images_X - reconstructed_X),[1,2,3]))\n",
        "\n",
        "            g_loss += opts.lambda_cycle * cycle_consistency_loss\n",
        "\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "\n",
        "            # Print the log info\n",
        "            if iteration % opts.log_step == 0:\n",
        "                print('Iteration [{:5d}/{:5d}] | d_real_loss: {:6.4f} | d_Y_loss: {:6.4f} | d_X_loss: {:6.4f} | '\n",
        "                    'd_fake_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
        "                      iteration, opts.train_iters, d_real_loss.item(), D_Y_loss.item(),\n",
        "                      D_X_loss.item(), d_fake_loss.item(), g_loss.item()))\n",
        "\n",
        "\n",
        "            # Save the generated samples\n",
        "            if iteration % opts.sample_every == 0:\n",
        "                cyclegan_save_samples(iteration, fixed_Y, fixed_X, G_YtoX, G_XtoY, opts)\n",
        "\n",
        "\n",
        "            # Save the model parameters\n",
        "            if iteration % opts.checkpoint_every == 0:\n",
        "                cyclegan_checkpoint(iteration, G_XtoY, G_YtoX, D_X, D_Y, opts)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return G_XtoY, G_YtoX, D_X, D_Y\n",
        "\n",
        "    return G_XtoY, G_YtoX, D_X, D_Y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuNFd6LNo0-o"
      },
      "source": [
        "# Part 3: Training Our GANs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRynU7a8kfCY"
      },
      "source": [
        "In this section we train and observe the results of the DCGAN and CycleGAN models that we implemented above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmQmyJDSRFKR"
      },
      "source": [
        "## 3.1. Training DCGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7hGiKe_GThN"
      },
      "source": [
        "### 3.1.1. Training DCGAN without Gradient Penalty\n",
        "We first train a DCGAN without a gradient penalty term in the discriminator loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LKaRF1jwhH7",
        "scrolled": true
      },
      "source": [
        "SEED = 11\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'image_size':32, \n",
        "              'g_conv_dim':32, \n",
        "              'd_conv_dim':64,\n",
        "              'noise_size':100,\n",
        "              'num_workers': 0,\n",
        "              'train_iters':20000,\n",
        "              'X':'Windows',  # options: 'Windows' / 'Apple'\n",
        "              'Y': None,\n",
        "              'lr':0.00025,\n",
        "              'beta1':0.5,\n",
        "              'beta2':0.999,\n",
        "              'batch_size':32, \n",
        "              'checkpoint_dir': 'results/checkpoints_gan',\n",
        "              'sample_dir': 'results/samples_gan',\n",
        "              'load': None,\n",
        "              'log_step':200,\n",
        "              'sample_every':200,\n",
        "              'checkpoint_every':9999,\n",
        "              'spectral_norm': False,\n",
        "              'gradient_penalty': False,\n",
        "              'd_train_iters': 1\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "G, D = train(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK-wpcx2OXvd"
      },
      "source": [
        "Now that we've finished training our model, let's take a look at the results!\n",
        "\n",
        "We first generate a gif that illustrates samples of our model's generated emojis over the course of its training every 200 iterations (total of 20000 iterations).\n",
        "\n",
        "We then depict side-by-side samples of the generated emojis after the $200^{th}$, $1000^{th}$, $5000^{th}$, $15000^{th}$,and $20000^{th}$ iterations for a closer look at the progression of our model's emoji-generating capabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SRDRlGPUMF_"
      },
      "source": [
        "generate_gif(\"results/samples_gan\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa0XtvM4_ba9"
      },
      "source": [
        "Image(open('results/samples_gan/anim.gif','rb').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y72uyBiPfYD"
      },
      "source": [
        "img1 = 'results/samples_gan/sample-000200'\n",
        "img2 = 'results/samples_gan/sample-001000'\n",
        "img3 = 'results/samples_gan/sample-005000'\n",
        "img4 = 'results/samples_gan/sample-015000'\n",
        "img5 = 'results/samples_gan/sample-020000'\n",
        "\n",
        "images = [img1, img2, img3, img4, img5]\n",
        "\n",
        "f, axs = plt.subplots(1,4,figsize=(40,40))\n",
        "\n",
        "for i in range(len(images)):\n",
        "  img = cv2.imread(images[i] +\".png\")\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  plt.subplot(1, 5, i+1)\n",
        "  plt.imshow(img)\n",
        "  plt.title(images[i][-6:])\n",
        "  plt.axis('off')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh3aBniMZQxo"
      },
      "source": [
        "We can clearly see that our DCGAN model has made significant progress towards generating new emojis from scratch over the course of its training. After about 200 iterations, the model barely surpasses outputs of indiscernable noise. We can see clear (albiet meaningless) patterns of horizontal and vertical lines of different colours. After 1000 iterations, the model beings to output variations in the shape of emojis, however the emojis are very noisy and visual details are not discernable. The output after 5000 iterations is the most interesting from the perspective of emoji clarity, uniqueness of outpus, and colour. While we begin to see clear emoji details, we still note the presence of artifacts. After both 15000 and 20000 iterations, it appears that the quality of generated emojis decreased as the same handful of emojis keep reappearing and they appear to be less detailed and riddled with more artifacts when compared to the output after 5000 iterations.\n",
        "\n",
        "In the next section, we attempt to train another DCGAN model in hopes of higher stability in the training output (if all goes well, we should expect to see higher quality emojis after 15000 and 20000 iterations)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFTRPP9vvd2i"
      },
      "source": [
        "### 3.1.2. Training DCGAN with Gradient Penalty\n",
        "We now train a new DCGAN model, but this time with the gradient penalty term added to the discriminator network's loss function. The motivation for this is to promote stability in the model's training. After we train the model, we observe the generated emojis like the previous section and then compare the stability of training between both models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9LC6C0rCKOe"
      },
      "source": [
        "SEED = 11\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'image_size':32, \n",
        "              'g_conv_dim':32, \n",
        "              'd_conv_dim':64,\n",
        "              'noise_size':100,\n",
        "              'num_workers': 0,\n",
        "              'train_iters':20000,\n",
        "              'X':'Windows',  # options: 'Windows' / 'Apple'\n",
        "              'Y': None,\n",
        "              'lr':0.00025,\n",
        "              'beta1':0.5,\n",
        "              'beta2':0.999,\n",
        "              'batch_size':32, \n",
        "              'checkpoint_dir': 'results/checkpoints_gan',\n",
        "              'sample_dir': 'results/samples_gan_with_gradpenalty',\n",
        "              'load': None,\n",
        "              'log_step':200,\n",
        "              'sample_every':200,\n",
        "              'checkpoint_every':9999,\n",
        "              'spectral_norm': False,\n",
        "              'gradient_penalty': True,\n",
        "              'd_train_iters': 1\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "G, D = train(args)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kYc_K7ArTiU"
      },
      "source": [
        "generate_gif(\"results/samples_gan_with_gradpenalty\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK1DoiSxjgK6"
      },
      "source": [
        "Image(open('results/samples_gan_with_gradpenalty/anim.gif','rb').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFB37JuebK31"
      },
      "source": [
        "img1 = 'results/samples_gan_with_gradpenalty/sample-000200'\n",
        "img2 = 'results/samples_gan_with_gradpenalty/sample-001000'\n",
        "img3 = 'results/samples_gan_with_gradpenalty/sample-005000'\n",
        "img4 = 'results/samples_gan_with_gradpenalty/sample-015000'\n",
        "img5 = 'results/samples_gan_with_gradpenalty/sample-020000'\n",
        "\n",
        "images = [img1, img2, img3, img4, img5]\n",
        "\n",
        "f, axs = plt.subplots(1,4,figsize=(20,20))\n",
        "\n",
        "for i in range(len(images)):\n",
        "  img = cv2.imread(images[i] +\".png\")\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  plt.subplot(1, 5, i+1)\n",
        "  plt.imshow(img)\n",
        "  plt.title(images[i][-6:])\n",
        "  plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh-Nk-6FlYp2"
      },
      "source": [
        "Like the results of the first DCGAN's output, the generated emojis after 200 iterations are barely a step above random noise. We begin to see some variation in emoji shapes and details by 1000 iterations. Some facial features such as eyes and mouths can even be observed at this point, but there are still quite a bit of artifacts. By 5000 iterations, we begin to observe some highly distinguised emojis. Like the DCGAN without a gradient penalty term, we also observe here the recurrence of some emojis in the 15000 and 20000 iteration samples, although with slightly higher quality. In all cases, while our models have clearly \"learned\" to create emojis in terms of colour and style, they certainly aren't the kinds of emojis that have any resembelence to meaningful symbols. Perhaps if we trained our models for a few thousand more iterations, we could begin to observe some emojis of symbolic value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQcxfQkXtBmC"
      },
      "source": [
        "We analyze one more thing before moving on to CycleGAN. The motivation for adding a gradient penalty term is to stabilize training (i.e. reduce large oscillations in the training loss over training iterations). We now observe if our gradient penalty implementation succeeded in this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ4lXAvCN3Yr"
      },
      "source": [
        "loss_gan = 'results/samples_gan/losses'\n",
        "loss_gan_with_gradpenalty = 'results/samples_gan_with_gradpenalty/losses'\n",
        "\n",
        "plt1, plt2 = cv2.imread(loss_gan +\".png\"), cv2.imread(loss_gan_with_gradpenalty+\".png\")\n",
        "plt1, plt2 = cv2.cvtColor(plt1, cv2.COLOR_BGR2RGB), cv2.cvtColor(plt2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "f, (ax1, ax2) = plt.subplots(1,2,figsize=(20,20))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(plt1)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title('GAN without Grad Penalty')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(plt2)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title('GAN with Grad Penalty')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLzupgQKn1wn"
      },
      "source": [
        "We focus on the training loss of the generator network (green line). While the loss for the DCGAN model with a gradient penalty did have much larger amplitudes in the oscillation of the loss early on in training, it can be clearly observed that the oscillations were much lower in amplitude versus the model without gradient penalty as the training continued. It is therefore shown that adding a gradient penalty did indeed help with stabilizing training in this instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cP7nl5NRJbu"
      },
      "source": [
        "## 3.2. Training CycleGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpT25aO2fgkm"
      },
      "source": [
        "We now move on to training our CycleGAN model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKlyfbuPDXDR"
      },
      "source": [
        "SEED = 4\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'image_size':32, \n",
        "              'g_conv_dim':32, \n",
        "              'd_conv_dim':32,\n",
        "              'init_zero_weights': False,\n",
        "              'num_workers': 0,\n",
        "              'train_iters':5000,\n",
        "              'X':'Apple',\n",
        "              'Y':'Windows',\n",
        "              'lambda_cycle': 0.75,\n",
        "              'lr':0.0003,\n",
        "              'beta1':0.3,\n",
        "              'beta2':0.999,\n",
        "              'batch_size':32, \n",
        "              'checkpoint_dir': 'results/checkpoints_cyclegan',\n",
        "              'sample_dir': 'results/samples_cyclegan',\n",
        "              'load': None,\n",
        "              'log_step':200,\n",
        "              'sample_every':200,\n",
        "              'checkpoint_every':1000\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "\n",
        "print_opts(args)\n",
        "G_XtoY, G_YtoX, D_X, D_Y = train(args)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvm5CswgfqrT"
      },
      "source": [
        "Now that we've finished training, lets take a look at the results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0RTTk0Xsbva"
      },
      "source": [
        "generate_gif(\"results/samples_cyclegan\", keyword='X-Y')\n",
        "generate_gif(\"results/samples_cyclegan\", keyword='Y-X')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRiISSl_fv5r"
      },
      "source": [
        "First, we look at our CycleGAN's generated Windows-style emoji given an Apple equivalent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm2KnCTAsdPZ"
      },
      "source": [
        "Image(open('results/samples_cyclegan/anim_X-Y.gif','rb').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwatKnEgf6Iu"
      },
      "source": [
        "Second, we look at our CycleGAN's generated Windows-style emoji given an Apple equivalent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8lmWrdMsqrL"
      },
      "source": [
        "Image(open('results/samples_cyclegan/anim_Y-X.gif','rb').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcmmLcuzgBIJ"
      },
      "source": [
        "We can clearly see from the 2 gifs above that our CycleGAN model learned quite effectively how to generate decent quality emojis (we do still note some artifacts however). While there are some minor discernable style differences between any given pair of emojis, they are largely very similar in style and so this model primarily learned how to reproduce as opposed to translate emojis. This model was trained with a `lambda_cycle` of 0.7. 5We note that when we decreased the `lambda_cycle` to about 0.015, the generated emojis were closer to the target style, although lower in quality (less distinguished features and more artifacts. One possible interpretation for this is that by increasing the `lambda_cycle`, the penalty for an incorrect translation of an emoji back to the original emoji also increases as well. So with a `lambda_cycle` of 0.75, we are able to generate higher fidelity emojis because of the higher penalty for a mismatch with the original. However, this came at the expense of a diminished distinction between the two styles of emojis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqoTQwQUKzNU"
      },
      "source": [
        "# References\n",
        "\n",
        "* Jimmy Ba.  Attention-Based Neural Machine Translation. *University of Toronto, CSC413*, 2020.\n",
        "\n",
        "* Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.\n",
        "Improved training of wasserstein gans. In *Advances in neural information processing systems*,\n",
        "pages 5767–5777, 2017.\n",
        "\n",
        "* Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of\n",
        "gans. *arXiv preprint arXiv:1705.07215*, 2017.\n",
        "\n",
        "* Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do\n",
        "actually converge? *arXiv preprint arXiv:1801.04406*, 2018.\n",
        "\n",
        "* Hoang Thanh-Tung, Truyen Tran, and Svetha Venkatesh. Improving generalization and stability\n",
        "of generative adversarial networks. *arXiv preprint arXiv:1902.03984*, 2019.\n"
      ]
    }
  ]
}